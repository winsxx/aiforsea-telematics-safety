{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab Safety Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure data visualization\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background and Objective\n",
    "Safety is an important aspect for online transportation. We want customers to feel safe riding Grab so that they could do other things on the way without worry. \n",
    "\n",
    "Customers could feel unsafe because of driver's behaviour or driving skill, eg: \n",
    "- Driver using unpopular shortcuts \n",
    "- Driver talk with another person on the phone or with customers\n",
    "- Driver keep seeing GPS and don't pay attention to the road\n",
    "- Sleepy \n",
    "- Speeding\n",
    "- Harsh acceleration, braking, or cornering\n",
    "- Run over speed bump/hole with high speed\n",
    "\n",
    "If we could quickly detect when the driver starts driving unsafely, we could remind the driver real-time to prevent something bad happen.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def read_csv_from_folder(path_pattern:str, file_limit:int=None):\n",
    "    \"\"\"Read csv files from path pattern. Only read file_limit files if it is defined\"\"\"\n",
    "    file_names = glob.glob(path_pattern)\n",
    "    file_content_list = []\n",
    "    \n",
    "    limit = len(file_names)\n",
    "    if file_limit is not None:\n",
    "        limit=min(file_limit, limit)\n",
    "        \n",
    "    print('Reading {} files from \"{}\" ...'.format(limit, path_pattern))\n",
    "    \n",
    "    for file_name in file_names[:limit]:\n",
    "        df = pd.read_csv(file_name, index_col=None, header=0)\n",
    "        file_content_list.append(df)\n",
    "    \n",
    "    contents = pd.concat(file_content_list, axis=0, ignore_index=True)\n",
    "    del file_content_list\n",
    "    return contents\n",
    "\n",
    "features_raw = read_csv_from_folder('../data/features/*.csv', file_limit=2)\n",
    "labels = read_csv_from_folder('../data/labels/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features data sample:')\n",
    "features_raw.head()\n",
    "print('Labels data sample:')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw.loc[features_raw.bookingID==1202590843006,:].sort_values(by='second').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature description\n",
    "### Booking id\n",
    "- Trip id\n",
    "- Possibly relate to service type (GrabCar/Bike) ?\n",
    "\n",
    "### Accuracy\n",
    "- Accuracy inferred by GPS in meters\n",
    "- Affect uncertainty level for GPS Bearing and Speed\n",
    "\n",
    "### Bearing\n",
    "- GPS bearing in degree\n",
    "- The degree of the GPS movement relative from North\n",
    "- Could relate with GPS accuracy. Less accurate means more uncertainty in the real speed.\n",
    "- Beware that 10 degree to 340 degrees is 30 degrees difference\n",
    "\n",
    "### Acceleration (x, y, z)\n",
    "- Accelerometer reading at a certain axis (m/s2)\n",
    "- [Youtube explaination about how Accelerometer works](https://www.youtube.com/watch?v=KZVgKu6v808)\n",
    "- Concern: how could we factor out gravity acceleration.\n",
    "- Concern: phone orientation\n",
    "\n",
    "### Gyro (x, y, z)\n",
    "- Gyroscope reading in certain axis (rad/s)\n",
    "- Measure angular velocity / speed of rotation\n",
    "- [Explaination about how Gyroscope works](https://learn.sparkfun.com/tutorials/gyroscope/all)\n",
    "- Concern: Gyroscope bias, usually caused by heat\n",
    "\n",
    "### Second\n",
    "- Time of the record by number of seconds\n",
    "- Remember that it is not in constant interval, eg: per 2s. If we want to use lag, we need to add for time delta or interpolate it.\n",
    "\n",
    "### Speed\n",
    "- Speed measured by GPS in m/s\n",
    "- Could relate with GPS accuracy. Less accurate means more uncertainty in the real speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train and test set by bookingID\n",
    "We only split the data to become two sets. Train and test set and we use the test set as the validation set as well. The data is split based on bookingID and before preprocessing to make sure there is no data leak from train to test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_ratio = 0.7\n",
    "\n",
    "all_booking_ids = features_raw.bookingID.unique()\n",
    "np.random.seed(1)\n",
    "train_booking_id = np.random.choice(all_booking_ids, \n",
    "                                    size = int(train_dataset_ratio * all_booking_ids.shape[0]), \n",
    "                                    replace=False)\n",
    "\n",
    "train_dataset = features_raw.loc[features_raw.bookingID.isin(train_booking_id), :].copy(deep=False)\n",
    "train_label = labels.loc[labels.bookingID.isin(train_booking_id), :].copy(deep=False)\n",
    "test_dataset = features_raw.loc[~features_raw.bookingID.isin(train_booking_id), :].copy(deep=False)\n",
    "test_label = labels.loc[~labels.bookingID.isin(train_booking_id), :].copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Safe and un-save trips')\n",
    "labels.label.value_counts()\n",
    "print('\\n')\n",
    "print('#BookingID with more than 1 rows in labels: {0}'.format((labels.bookingID.value_counts() > 1).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle double label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproces_label(labels):\n",
    "    return labels.groupby(['bookingID']).max().reset_index().copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = preproces_label(train_label)\n",
    "test_label = preproces_label(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence and Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_sorted(dataset):\n",
    "    dataset_copy = dataset.copy(deep=False)\n",
    "    \n",
    "    dataset_copy['sequence'] = dataset_copy[\n",
    "        ['bookingID', 'second']\n",
    "    ].groupby('bookingID').rank(ascending=True, method='first')\n",
    "\n",
    "    dataset_copy = dataset_copy.sort_values(by=['bookingID', 'second'])\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gyroscope Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually phone is not rotating all the time and the value of gyroscope will be 0.\n",
    "- [Knowing that there is a bias of gyroscope reading](https://base.xsens.com/hc/en-us/articles/209611089-Understanding-Sensor-Bias-offset-), we could use mean to find the expected reading while the phone is in stable position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(features_raw.loc[features_raw.bookingID==1477468749954,['gyro_x']])\n",
    "print('Gyroscope reading, x-axis bias:', features_raw.loc[features_raw.bookingID==1477468749954,['gyro_x']].mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gyro_data_enrich(dataset):\n",
    "    enriched_dataset = dataset.copy(deep=False)\n",
    "    enriched_dataset = ensure_sorted(enriched_dataset)\n",
    "    \n",
    "    gyro_cols = ['gyro_x', 'gyro_y', 'gyro_z']\n",
    "    \n",
    "    # Find gyroscope bias / stable values\n",
    "    for col in gyro_cols:\n",
    "        if (col+'_stable') in enriched_dataset.columns:\n",
    "            continue\n",
    "        agg_stable = enriched_dataset.groupby('bookingID')[col].mean().reset_index()\n",
    "        agg_stable.columns = ['bookingID', col+'_stable']\n",
    "        enriched_dataset = pd.merge(enriched_dataset, agg_stable, how='left', on='bookingID', validate='m:1', copy=False)\n",
    "\n",
    "    # Gyroscope filtered / calibrated values\n",
    "    for col in gyro_cols:\n",
    "        if (col+'_filtered') in enriched_dataset.columns:\n",
    "            continue\n",
    "        enriched_dataset[col+'_filtered'] = enriched_dataset[col] - enriched_dataset[col+'_stable']\n",
    "    \n",
    "    # Gyroscope magnitude of calibrated values\n",
    "    enriched_dataset['gyro_filtered_magnitude'] = np.sqrt(enriched_dataset['gyro_x_filtered']**2 + \\\n",
    "                                                          enriched_dataset['gyro_y_filtered']**2 + \\\n",
    "                                                          enriched_dataset['gyro_z_filtered']**2)\n",
    "    \n",
    "    # Gyroscope magnitude standard deviation\n",
    "    agg_std = enriched_dataset.groupby('bookingID')['gyro_filtered_magnitude'].std().reset_index()\n",
    "    agg_std.columns = ['bookingID', 'gyro_filtered_std']\n",
    "    enriched_dataset = pd.merge(enriched_dataset, agg_std, how='left', on='bookingID', validate='m:1', copy=False)\n",
    "        \n",
    "    return enriched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gyro_data_enrich(train_dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerometer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accelerometer readings depends on gravity\n",
    "- [Phone orientation](https://www.digikey.com/en/articles/techzone/2011/may/using-an-accelerometer-for-inclination-sensing) could change over time and change the gravity acceleration for each axis\n",
    "\n",
    "Could we?\n",
    "- Handle accelerometer bias? Gravity is not always 9.8. It depends on altitude and accelerometer bias. \n",
    "- Distinguish between vehicle movement and user moving the phone?\n",
    "- Normalize all data assuming all phones are having the same orientation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accel_data_enrich(dataset, smoothing: int=3):\n",
    "    enriched_dataset = dataset.copy(deep=False)\n",
    "    enriched_dataset = ensure_sorted(enriched_dataset)\n",
    "    \n",
    "    accel_cols = pd.Series(['acceleration_x', 'acceleration_y', 'acceleration_z'])\n",
    "    \n",
    "    # Rolling mean of accleration data to find gravity\n",
    "    rolling_mean_data = enriched_dataset.groupby('bookingID').apply(\n",
    "        lambda x: x[\n",
    "            accel_cols\n",
    "        ].rolling(window=smoothing, min_periods=1, center=True).mean())\n",
    "    rolling_mean_data.columns = accel_cols + '_gravity'\n",
    "    enriched_dataset = pd.concat([enriched_dataset, rolling_mean_data], axis=1, verify_integrity=True)\n",
    "    \n",
    "    # Acceleration magnitude\n",
    "    enriched_dataset['acceleration_magnitude'] = np.sqrt(enriched_dataset['acceleration_x']**2 + \\\n",
    "                                                         enriched_dataset['acceleration_y']**2 + \\\n",
    "                                                         enriched_dataset['acceleration_z']**2) \n",
    "    \n",
    "    # Current acceleration vs gravity diff\n",
    "    for col in accel_cols:\n",
    "        enriched_dataset[col+'_gravity_diff'] = enriched_dataset[col] - enriched_dataset[col+'_gravity']\n",
    "    enriched_dataset['acceleration_gravity_diff_magnitude'] = np.sqrt(enriched_dataset['acceleration_x_gravity_diff']**2 + \\\n",
    "                                                                      enriched_dataset['acceleration_y_gravity_diff']**2 + \\\n",
    "                                                                      enriched_dataset['acceleration_z_gravity_diff']**2) \n",
    "    \n",
    "    # Acceleration magnitude standard deviation\n",
    "    agg_std = enriched_dataset.groupby('bookingID')['acceleration_magnitude', 'acceleration_gravity_diff_magnitude'].std().reset_index()\n",
    "    agg_std.columns = ['bookingID', 'acceleration_std', 'acceleration_gravity_diff_std']\n",
    "    enriched_dataset = pd.merge(enriched_dataset, agg_std, how='left', on='bookingID', validate='m:1', copy=False)\n",
    "    \n",
    "    # Phone orientation\n",
    "    enriched_dataset['orientation_theta'] = np.arctan(enriched_dataset.acceleration_x_gravity / \\\n",
    "        np.sqrt(enriched_dataset.acceleration_y_gravity**2 + enriched_dataset.acceleration_z_gravity**2)) / np.pi * 360\n",
    "    enriched_dataset['orientation_psi'] = np.arctan(enriched_dataset.acceleration_y_gravity / \\\n",
    "        np.sqrt(enriched_dataset.acceleration_x_gravity**2 + enriched_dataset.acceleration_z_gravity**2)) / np.pi * 360\n",
    "    enriched_dataset['orientation_phi'] = np.arctan( np.sqrt(enriched_dataset.acceleration_x_gravity**2 + enriched_dataset.acceleration_y_gravity**2) / \\\n",
    "        enriched_dataset.acceleration_z_gravity ) / np.pi * 360\n",
    "    \n",
    "    return enriched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accel_data_enrich(train_dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Difference Data and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_data_enrich(dataset):\n",
    "    enriched_dataset = dataset.copy(deep=False)\n",
    "    enriched_dataset = ensure_sorted(enriched_dataset)\n",
    "    \n",
    "    # Construct diff\n",
    "    diff_data = enriched_dataset.groupby('bookingID')['second','Bearing','Speed'].diff()\n",
    "    diff_data = diff_data.rename(columns = lambda x: x + '_diff')\n",
    "    \n",
    "    # Modify Bearing diff to -180 to 180 \n",
    "    diff_data.Bearing_diff = diff_data.Bearing_diff\n",
    "    diff_data.Bearing_diff[diff_data.Bearing_diff < -180.0] += 180\n",
    "    diff_data.Bearing_diff[diff_data.Bearing_diff > 180.0] -= 180\n",
    "\n",
    "    # Difference / second (normalization)\n",
    "    diff_data['Bearing_dps'] = diff_data['Bearing_diff'] / diff_data['second_diff']\n",
    "    diff_data['Speed_dps'] = diff_data['Speed_diff'] / diff_data['second_diff']\n",
    "    \n",
    "    # Combine\n",
    "    diff_data = diff_data.fillna(0)\n",
    "    enriched_dataset = pd.concat([enriched_dataset, diff_data], axis=1, verify_integrity=True)\n",
    "    \n",
    "    # Combine accuracy of two sequence\n",
    "    acc_sum = enriched_dataset.groupby('bookingID')['Accuracy']\\\n",
    "       .rolling(window=2, min_periods=1).sum().reset_index(drop=True).tolist()\n",
    "    enriched_dataset['Accuracy_sum'] = acc_sum\n",
    "    \n",
    "    return enriched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# diff_data_enrich(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_data(preprocessed_dataset):\n",
    "    features_max = ['gyro_filtered_magnitude',\n",
    "                    'acceleration_magnitude',\n",
    "                    'Speed',\n",
    "                    'Bearing_dps',\n",
    "                    'Speed_dps',\n",
    "                    'Accuracy_sum',\n",
    "                    'second',\n",
    "                    'sequence',\n",
    "                    'acceleration_x_gravity_diff',\n",
    "                    'acceleration_y_gravity_diff',\n",
    "                    'acceleration_z_gravity_diff',\n",
    "                    'acceleration_gravity_diff_magnitude',\n",
    "                    'gyro_x_filtered',\n",
    "                    'gyro_y_filtered',\n",
    "                    'gyro_z_filtered']\n",
    "    \n",
    "    agg_max = preprocessed_dataset.groupby('bookingID')[features_max].max().reset_index()\n",
    "    agg_max.columns = ['bookingID'] +  (pd.Series(features_max) + '_max').tolist()\n",
    "\n",
    "    features_std = ['gyro_filtered_magnitude', 'acceleration_gravity_diff_magnitude']\n",
    "    agg_std = preprocessed_dataset.groupby('bookingID')[features_std].std().reset_index()\n",
    "    agg_std.columns = ['bookingID'] +  (pd.Series(features_std) + '_std').tolist()\n",
    "    \n",
    "    agg_data = pd.merge(agg_max, agg_std, on='bookingID', validate='1:1')\n",
    "    agg_data['second_sequence_ratio'] = agg_data['second_max'] / agg_data['sequence_max'].astype(float)\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    dataset = gyro_data_enrich(dataset)\n",
    "    dataset = accel_data_enrich(dataset, smoothing=5)\n",
    "    dataset = diff_data_enrich(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_prep = preprocess(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chart_trip(dataset, booking_id):\n",
    "    booking_id_data = dataset.loc[dataset.bookingID==booking_id,:].sort_values(by='second')\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplots_adjust(hspace = .001)\n",
    "    \n",
    "    # Acceleration\n",
    "    booking_id_acc = booking_id_data[\n",
    "        ['second','acceleration_x', 'acceleration_y','acceleration_z', 'acceleration_magnitude']\n",
    "    ].melt(id_vars=[\"second\"], var_name=\"axis\", value_name=\"value\")\n",
    "    ax1 = plt.subplot('311')\n",
    "    \n",
    "    plt.title(\"Measured data for booking ID: {}\".format(booking_id))\n",
    "    sns.lineplot(x=\"second\", y=\"value\", hue='axis', data=booking_id_acc, ax=ax1, marker=\"o\");\n",
    "    \n",
    "    # Gyroscope\n",
    "    booking_id_gyro = booking_id_data[\n",
    "        ['second', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    ].melt(id_vars=[\"second\"], var_name=\"axis\", value_name=\"value\")\n",
    "    ax2 = plt.subplot('312')\n",
    "    sns.lineplot(x=\"second\", y=\"value\", hue='axis', data=booking_id_gyro, ax=ax2, markers=True, marker=\"o\");\n",
    "    \n",
    "    # Speed\n",
    "    booking_id_speed = booking_id_data[\n",
    "        ['second', 'Speed', 'Accuracy']\n",
    "    ].melt(id_vars=[\"second\"], var_name=\"type\", value_name=\"value\")\n",
    "    ax3 = plt.subplot('313')\n",
    "    sns.lineplot(x='second', y=\"value\", hue='type', data=booking_id_speed, ax=ax3, markers=True, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_trip(train_dataset_prep, 1477468749954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of non-safe trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = train_label.bookingID[train_label.label == 1].sample(5, random_state=2)\n",
    "for id in samp:\n",
    "    chart_trip(train_dataset_prep, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of safe trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = train_label.bookingID[train_label.label == 0].sample(5, random_state=2)\n",
    "for id in samp:\n",
    "    chart_trip(train_dataset_prep, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analytics_features = ['gyro_filtered_magnitude',\n",
    "                      'acceleration_magnitude',\n",
    "                      'Speed',\n",
    "                      'Bearing_dps',\n",
    "                      'Speed_dps',\n",
    "                      'second_diff',\n",
    "                      'second',\n",
    "                      'Accuracy_sum',\n",
    "                      'acceleration_x', \n",
    "                      'acceleration_y', \n",
    "                      'acceleration_z',\n",
    "                      'acceleration_x_gravity_diff',\n",
    "                      'acceleration_y_gravity_diff',\n",
    "                      'acceleration_z_gravity_diff',\n",
    "                      'acceleration_gravity_diff_magnitude',\n",
    "                      'gyro_x_filtered',\n",
    "                      'gyro_y_filtered',\n",
    "                      'gyro_z_filtered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analytics_mean_corr = train_dataset_prep.groupby('bookingID')[analytics_features].mean().reset_index()\n",
    "analytics_mean_corr = pd.merge(analytics_mean_corr, train_label, on='bookingID')\n",
    "analytics_mean_corr = analytics_mean_corr.corr()\n",
    "\n",
    "plt.figure( figsize=(15,5) )\n",
    "ax1 = plt.subplot2grid((1, 4), (0, 0), colspan=2)\n",
    "ax2 = plt.subplot2grid((1, 4), (0, 3), colspan=1)\n",
    "sns.heatmap(analytics_mean_corr, ax=ax1)\n",
    "sns.heatmap(pd.DataFrame(analytics_mean_corr.loc[analytics_mean_corr.index != 'label','label']), annot=True, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation table shows that mean/average data is less effective to determine a trip is save or unsafe. The unsafe tracking event maybe only recorded 1 time or maybe < 5% of the trip. But, acceleration and gyroscope magnitude data stand out here. How many % of the trips where the driver consistently drive unsafely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_max_corr = train_dataset_prep.groupby('bookingID')[analytics_features].max().reset_index()\n",
    "analytics_max_corr = pd.merge(analytics_max_corr, train_label, on='bookingID')\n",
    "analytics_max_corr = analytics_max_corr.corr()\n",
    "\n",
    "plt.figure( figsize=(15,5) )\n",
    "ax1 = plt.subplot2grid((1, 4), (0, 0), colspan=2)\n",
    "ax2 = plt.subplot2grid((1, 4), (0, 3), colspan=1)\n",
    "sns.heatmap(analytics_max_corr, ax=ax1)\n",
    "sns.heatmap(pd.DataFrame(analytics_max_corr.loc[analytics_max_corr.index != 'label','label']), annot=True, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard deviation correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_std_corr = train_dataset_prep.groupby('bookingID')[analytics_features].std().reset_index()\n",
    "analytics_std_corr = pd.merge(analytics_std_corr, train_label, on='bookingID')\n",
    "analytics_std_corr = analytics_std_corr.corr()\n",
    "\n",
    "plt.figure( figsize=(15,5) )\n",
    "ax1 = plt.subplot2grid((1, 4), (0, 0), colspan=2)\n",
    "ax2 = plt.subplot2grid((1, 4), (0, 3), colspan=1)\n",
    "sns.heatmap(analytics_std_corr, ax=ax1)\n",
    "sns.heatmap(pd.DataFrame(analytics_std_corr.loc[analytics_std_corr.index != 'label','label']), annot=True, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gyro and acceleration standard deviation is highly correlated. The hypothesis is when there is angular velocity (gyroscope reading), the phone orientation is changing. Hence, the direction of gravity relative to the phone is changing and the reading for each accelerometer axis is changing too. \n",
    "\n",
    "Because it is highly correlated, we will just use subset of it:\n",
    "- gyroscope filter magnitude's standard deviation\n",
    "- acceleration gravity diff magnitude's standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could we cluster the trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg_data = aggregate_data(train_dataset_prep)\n",
    "train_agg_data = pd.merge(train_agg_data, train_label, on='bookingID', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = train_agg_data.columns[train_agg_data.columns.str.contains(\"max|std\")]\n",
    "\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "x = std_scaler.fit_transform(train_agg_data[features])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pc = pca.fit_transform(x)\n",
    "pc_df = pd.DataFrame(data = pc, columns = ['pc1', 'pc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=\"pc1\", \n",
    "                y=\"pc2\", \n",
    "                hue=\"label\",\n",
    "                data=pd.concat([train_agg_data, pc_df], axis=1, verify_integrity=True).sample(2000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **no visually standout clusters** after we reduce the features to 2-dimension with PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does trip duration effects safety?\n",
    "There are two possibilities (hypothesis)\n",
    "1. Longer trips increase the probability to encounter non-safe events\n",
    "2. There are trips with a duration of < 2 minutes. Probably short duration trips are cancelled trips and that is why most of them are safe trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log2(train_agg_data.loc[(train_agg_data.label == 0) & (train_agg_data.second_max < 7200)].second_max), \n",
    "             label='safe', color='green')\n",
    "sns.distplot(np.log2(train_agg_data.loc[(train_agg_data.label == 1) & (train_agg_data.second_max < 7200)].second_max), \n",
    "             label='not safe', color='red')\n",
    "plt.xlim(xmax=15)\n",
    "plt.xlabel('log2(trip duration in second)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins =  [i for i in range(0,(40+1),2)] # List of 0, 2,4,6 .., 40\n",
    "df_pct_area = train_agg_data.loc[train_agg_data.second_max < (40*60), ['bookingID','second_max', 'label']]\n",
    "df_pct_area['duration_min'] = df_pct_area.second_max / 60.0\n",
    "df_pct_area['2min_bin'] = pd.cut(df_pct_area['duration_min'], bins=bins, labels=bins[:-1])\n",
    "df_pct_area = pd.pivot_table(df_pct_area, values='bookingID', aggfunc=lambda x: x.shape[0], index='2min_bin', columns='label')\n",
    "df_pct_area = df_pct_area.divide(df_pct_area.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the plot\n",
    "plt.stackplot(range(0,40,2),  df_pct_area[1],  df_pct_area[0], labels=['not safe','safe'], \n",
    "              colors=['#83d0c9','#ff6f69'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.margins(0,0)\n",
    "plt.title('Percentage Area Chart of Trip Safety')\n",
    "plt.xlabel('duration (min)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing this chart, because of the un-safe trip proportion increase linearly, we believe more on the first hypothesis over the other one. Longer trips increase the probability to encounter non-safe event. This belief is strengthened when we see the other side which is almost all trips after 40 minutes are non-safe trips.\n",
    "\n",
    "Additionally, after seeing the smoothness of the chart, this data seems like a little bit odd. Seeing the pattern, there is a hypothesis that the label is produced by other machine learning model which have an additive effect. The model predicts each segment of the trip and adds up the unsafeness. I think using trip duration feature to train and predict is like some kind of \"cheat\" and we should not use it in the real case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does tracking rate have corellation with label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_agg_data.loc[(train_agg_data.label == 0) & (train_agg_data.second_sequence_ratio < 40.0)].second_sequence_ratio, \n",
    "             label='safe', color='green')\n",
    "sns.distplot(train_agg_data.loc[(train_agg_data.label == 1) & (train_agg_data.second_sequence_ratio < 40.0)].second_sequence_ratio, \n",
    "             label='not safe', color='red')\n",
    "plt.xlabel('Distribution of duration between sequence')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost no difference in event logging rate between labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Our goal is to make the model which is capable to find the pattern of non-safe event like:\n",
    "- Driver using unpopular shortcuts \n",
    "- Driver talk with other person in phone or with customers\n",
    "- Driver keep seeing GPS and don't pay attention to the road. \n",
    "- Sleepy \n",
    "- Speeding\n",
    "- Harsh acceleration, braking, or cornering\n",
    "- Run over speed bump / hole with high speed.\n",
    "\n",
    "There are 4 rough ideas to approach this problem:\n",
    "1. Learning on the summary of a trip\n",
    "2. Stacking two models. The first model to detect non-safe events. The second model to summarize it.\n",
    "3. We could also use CNN instead of stacking manually. We only need to learn patterns from sensor events which are close to each other. CNN fits the purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def combine_pred_label(prediction_df, label_df):\n",
    "    \"\"\"Combine two DataFrame, each DataFrame should contains 'bookingID' column.\"\"\"\n",
    "    return pd.merge(prediction_df, label_df, how='left', on='bookingID', validate='1:1')\n",
    "\n",
    "def plot_roc(prediction_df, label_df):\n",
    "    \"\"\"Return ROC plot given prediction and label DataFrame. Both should have 'bookingID' column.\"\"\"\n",
    "    pred_label_df = combine_pred_label(prediction_df, label_df)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(pred_label_df.label, pred_label_df.prediction)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate(prediction_df, label_df):\n",
    "    \"\"\"Return AUC evaluation given prediction and label DataFrame. Both should have 'bookingID' column.\"\"\"\n",
    "    pred_label_df = combine_pred_label(prediction_df, label_df)\n",
    "    return roc_auc_score(pred_label_df.label, pred_label_df.prediction)\n",
    "                         \n",
    "def generate_second_dataset(dataset, prediciton, n_column=5):\n",
    "    \"\"\"\n",
    "    Generate per booking secondary dataset from per event unsafeness prediction.\n",
    "    Return top n_columns unsafeness for each bookingID\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset -- DataFrame contains bookingID\n",
    "    prediction -- List / Series with length equal to dataset # rows. Each indicates unsafeness. \n",
    "    n_column -- number of column generated\n",
    "    \"\"\"\n",
    "    sec_data = pd.DataFrame(data={'bookingID':dataset.bookingID, 'row_prob': prediciton})\n",
    "    sec_data['rank'] = sec_data.groupby('bookingID').rank(ascending=False, method='first')\n",
    "    sec_data = sec_data.loc[sec_data['rank'] <= n_column, :]\n",
    "    sec_data = pd.pivot_table(data=sec_data, \n",
    "                              values='row_prob', \n",
    "                              index='bookingID', \n",
    "                              columns='rank', \n",
    "                              fill_value=0).reset_index()\n",
    "    sec_data.columns=['bookingID'] + ['val_' + str(i) for i in range(1, (n_column + 1))]\n",
    "    return sec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Baseline: Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "prediction_df = pd.DataFrame({\n",
    "    'bookingID': test_dataset.bookingID.unique(),\n",
    "    'prediction': np.random.random(size=test_dataset.bookingID.unique().shape[0])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(prediction_df, test_label)\n",
    "plot_roc(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest - Aggregated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_prep = preprocess(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg_data = aggregate_data(train_dataset_prep)\n",
    "train_agg_data = pd.merge(train_agg_data, train_label, on='bookingID', validate='1:1')\n",
    "\n",
    "test_dataset_prep = preprocess(test_dataset)\n",
    "test_agg_data = aggregate_data(test_dataset_prep)\n",
    "\n",
    "features = train_agg_data.columns[train_agg_data.columns.str.contains(\"max|std|ratio\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = RandomForestClassifier(n_estimators=200, random_state=0, min_samples_leaf=75)\n",
    "cls.fit(train_agg_data[features], train_agg_data.label)\n",
    "pred = cls.predict_proba(test_agg_data[features])\n",
    "pred = pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "prediction_df = pd.DataFrame(data={'bookingID':test_agg_data.bookingID, 'prediction': pred})\n",
    "print('AUC:',evaluate(prediction_df, test_label))\n",
    "plot_roc(prediction_df, test_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM - Logistic Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_prep = preprocess(train_dataset)\n",
    "train_dataset_prep = pd.merge(train_dataset_prep, train_label, on='bookingID', validate='m:1')\n",
    "test_dataset_prep = preprocess(test_dataset)\n",
    "test_dataset_prep = pd.merge(test_dataset_prep, test_label, on='bookingID', validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prep.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Accuracy', 'gyro_x_filtered', 'gyro_y_filtered', 'gyro_z_filtered', 'Speed', 'gyro_filtered_magnitude', \n",
    "            'acceleration_magnitude', 'Bearing_dps', 'Speed_dps', 'Accuracy_sum', \n",
    "            'acceleration_x_gravity_diff', 'acceleration_y_gravity_diff', 'acceleration_z_gravity_diff', \n",
    "            'acceleration_gravity_diff_magnitude', 'orientation_theta', \n",
    "            'orientation_psi', 'orientation_phi', 'second', 'acceleration_gravity_diff_std',\n",
    "            'acceleration_std','gyro_filtered_std']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = lgb.LGBMClassifier(boosting_type='dart', \n",
    "                         objective='binary', \n",
    "                         max_depth=6, \n",
    "                         n_estimator=100,\n",
    "                         learning_rate=0.1, \n",
    "                         max_bin=100, \n",
    "                         num_leaves=70, \n",
    "                         lambda_l1=0.4,\n",
    "                         min_data_in_leaf=150,\n",
    "                         metric='auc')\n",
    "\n",
    "cls.fit(train_dataset_prep[features], train_dataset_prep.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this\n",
    "y_pred = cls.predict_proba(test_dataset_prep[features])\n",
    "y_pred = y_pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "roc_auc_score(test_dataset_prep.label, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred_first = cls.predict_proba(train_dataset_prep[features])\n",
    "train_pred_first = train_pred_first[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "\n",
    "test_pred_first = cls.predict_proba(test_dataset_prep[features])\n",
    "test_pred_first = test_pred_first[:,np.argwhere(cls.classes_==1)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_sec = generate_second_dataset(train_dataset_prep, train_pred_first, n_column=5)\n",
    "train_sec = pd.merge(train_sec, train_label, on='bookingID')\n",
    "sec_features = train_sec.columns[train_sec.columns.str.contains('val')]\n",
    "sec_reg = LogisticRegression(random_state=0, penalty='l2', C=0.1)\n",
    "sec_reg.fit(train_sec[sec_features], train_sec.label)\n",
    "\n",
    "test_sec = generate_second_dataset(test_dataset_prep, test_pred_first, n_column=5)\n",
    "test_sec = pd.merge(test_sec, test_label, on='bookingID')\n",
    "y_pred = sec_reg.predict_proba(test_sec[sec_features])\n",
    "y_pred = y_pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "prediction_df = pd.DataFrame(data={'bookingID':test_sec.bookingID, 'prediction': y_pred})\n",
    "\n",
    "evaluate(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(prediction_df.prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. CNN\n",
    "We will use [1D CNN for time series data / text](https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_agg_data.sequence_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, BatchNormalization, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, AveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.regularizers import l1\n",
    "\n",
    "def to_keras_input(dataset, features, maxlen=200) -> (list, pd.DataFrame):\n",
    "    dataset_seq = dataset[\n",
    "        list(set(['bookingID','second'] + features))\n",
    "    ].groupby('bookingID').apply(\n",
    "        lambda x: x.sort_values(by='second')[features].values.tolist()\n",
    "    )\n",
    "    booking_ids = pd.DataFrame({'idx': range(len(dataset_seq.index)) ,'bookingID':dataset_seq.index})\n",
    "    dataset_seq = pad_sequences(dataset_seq, maxlen=maxlen, padding='pre', dtype=float, truncating='pre', value=0.0)\n",
    "    return dataset_seq, booking_ids\n",
    "\n",
    "def create_model_cnn1(dataset):\n",
    "    num_seq = len(dataset[0])\n",
    "    num_features = len(dataset[0][0])\n",
    "    \n",
    "    model_m = Sequential()\n",
    "    model_m.add(Conv1D(16, 3, activation='relu', input_shape=(num_seq, num_features)))\n",
    "    model_m.add(MaxPooling1D(3))\n",
    "    model_m.add(Conv1D(32, 3, activation='relu', input_shape=(num_seq, num_features)))\n",
    "    model_m.add(MaxPooling1D(3))\n",
    "    model_m.add(Conv1D(64, 3, activation='relu', input_shape=(num_seq, num_features)))\n",
    "    model_m.add(MaxPooling1D(3))\n",
    "    model_m.add(GlobalAveragePooling1D())\n",
    "    model_m.add(Dropout(0.2))\n",
    "    model_m.add(Dense(16, activation='sigmoid'))\n",
    "    model_m.add(Dense(1, activation='sigmoid'))\n",
    "    print(model_m.summary())\n",
    "    return model_m\n",
    "\n",
    "def create_model_cnn_2(dataset):\n",
    "    num_seq = len(dataset[0])\n",
    "    num_features = len(dataset[0][0])\n",
    "    \n",
    "    inpt = Input(shape=(num_seq, num_features))\n",
    "    \n",
    "    convs = []\n",
    "    \n",
    "    conv1 = Conv1D(8, 1, activation='relu')(inpt)\n",
    "    pool1 = GlobalMaxPooling1D()(conv1)\n",
    "    convs.append(pool1)\n",
    "    \n",
    "    conv2 = Conv1D(8, 3, activation='relu')(inpt)\n",
    "    pool2_1 = AveragePooling1D(pool_size=5)(conv2)\n",
    "    conv2_1 = Conv1D(16, 3, activation='relu')(pool2_1)\n",
    "    pool2_2 = GlobalMaxPooling1D()(conv2_1)\n",
    "    convs.append(pool2_2)\n",
    "    \n",
    "    \n",
    "    out = Concatenate()(convs)\n",
    "    first_segment_model = Model(inputs=[inpt], outputs=[out])\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(first_segment_model)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    print(first_segment_model.summary())\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['acceleration_x', 'acceleration_y', 'acceleration_z', 'acceleration_gravity_diff_magnitude', \n",
    "            'Bearing', 'gyro_x_filtered', 'gyro_y_filtered', 'gyro_z_filtered', 'gyro_filtered_magnitude',\n",
    "            'Speed', 'Accuracy', 'second', 'second_diff', 'orientation_theta', 'orientation_psi', 'orientation_phi']\n",
    "\n",
    "train_feature_cnn = train_dataset_prep.copy()\n",
    "\n",
    "train_feature_cnn[['acceleration_x', 'acceleration_y', 'acceleration_z']] = \\\n",
    "    train_feature_cnn[['acceleration_x', 'acceleration_y', 'acceleration_z']] / 10.0\n",
    "train_feature_cnn['Bearing'] = train_feature_cnn['Bearing'] / 360.0\n",
    "train_feature_cnn[['orientation_theta', 'orientation_psi', 'orientation_phi']] = \\\n",
    "    train_feature_cnn[['orientation_theta', 'orientation_psi', 'orientation_phi']] / 180.0\n",
    "train_feature_cnn['Speed'] = train_feature_cnn['Speed'] / 35\n",
    "train_feature_cnn['second'] = train_feature_cnn['second'] / 1750\n",
    "train_feature_cnn['second_diff'] = train_feature_cnn['second_diff'] / 30\n",
    "train_feature_cnn['Accuracy'] = train_feature_cnn['Accuracy'] / 15\n",
    "\n",
    "train_feature_cnn, train_booking_ids = to_keras_input(train_feature_cnn, features, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_cnn_2(train_feature_cnn)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['binary_accuracy'])\n",
    "train_seqlabel = pd.merge(train_booking_ids, train_label, on='bookingID').label\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(monitor='binary_accuracy', patience=3)\n",
    "]\n",
    "history = model.fit(train_feature_cnn,\n",
    "                    np.array(train_seqlabel).reshape((-1,1)),\n",
    "                    batch_size=4,\n",
    "                    epochs=50,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_cnn = test_dataset_prep.copy()\n",
    "\n",
    "test_feature_cnn[['acceleration_x', 'acceleration_y', 'acceleration_z']] = \\\n",
    "    test_feature_cnn[['acceleration_x', 'acceleration_y', 'acceleration_z']] / 10.0\n",
    "test_feature_cnn[['gyro_x_filtered', 'gyro_y_filtered', 'gyro_z_filtered']] = \\\n",
    "    test_feature_cnn[['gyro_x_filtered', 'gyro_y_filtered', 'gyro_z_filtered']] \n",
    "test_feature_cnn['Bearing'] = test_feature_cnn['Bearing'] / 360.0\n",
    "test_feature_cnn[['orientation_theta', 'orientation_psi', 'orientation_phi']] = \\\n",
    "    test_feature_cnn[['orientation_theta', 'orientation_psi', 'orientation_phi']] / 180.0\n",
    "test_feature_cnn['Speed'] = test_feature_cnn['Speed'] / 35.0\n",
    "test_feature_cnn['second'] = test_feature_cnn['second'] / 1750.0\n",
    "test_feature_cnn['second_diff'] = test_feature_cnn['second_diff'] / 30.0\n",
    "test_feature_cnn['Accuracy'] = test_feature_cnn['Accuracy'] / 15.0\n",
    "\n",
    "test_feature_cnn, test_booking_ids = to_keras_input(test_feature_cnn, features, maxlen=200)\n",
    "\n",
    "pred = model.predict_proba(test_feature_cnn)\n",
    "prediction_df = pd.DataFrame(data={'bookingID': test_booking_ids.bookingID, 'prediction': pred[:,0]})\n",
    "evaluate(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pd.Series(pred[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNN - Random Forest Aggregate (Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "\n",
    "for layer in model.layers[:-1]:\n",
    "    cnn_model.add(layer)    \n",
    "\n",
    "# Freeze the layers \n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "test_cnn_emb = cnn_model.predict_proba(test_feature_cnn)\n",
    "train_cnn_emb = cnn_model.predict_proba(train_feature_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_features = train_agg_data.columns[train_agg_data.columns.str.contains(\"max|std|ratio\")]\n",
    "\n",
    "train_data_stack = pd.concat([\n",
    "    pd.Series(train_booking_ids.bookingID), \n",
    "    train_agg_data[agg_features],\n",
    "    pd.DataFrame(train_cnn_emb, columns=['cnn_result_'+ str(i) for i in range(len(train_cnn_emb[0]))])\n",
    "], axis=1)\n",
    "\n",
    "test_data_stack = pd.concat([\n",
    "    pd.Series(test_booking_ids.bookingID), \n",
    "    test_agg_data[agg_features],\n",
    "    pd.DataFrame(test_cnn_emb, columns=['cnn_result_'+ str(i) for i in range(len(test_cnn_emb[0]))])\n",
    "], axis=1)\n",
    "\n",
    "train_data_stack = pd.merge(train_data_stack, train_label, on='bookingID')\n",
    "train_data_stack_features = train_data_stack.columns[train_data_stack.columns != 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_clf = RandomForestClassifier(n_estimators=200, random_state=0, min_samples_leaf=75)\n",
    "stack_clf.fit(train_data_stack[train_data_stack_features], train_data_stack.label)\n",
    "pred = stack_clf.predict_proba(test_data_stack[train_data_stack_features])\n",
    "pred = pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "\n",
    "prediction_df = pd.DataFrame(data={'bookingID':test_data_stack.bookingID, 'prediction': pred})\n",
    "print('AUC:',evaluate(prediction_df, test_label))\n",
    "plot_roc(prediction_df, test_label);\n",
    "\n",
    "evaluate(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
