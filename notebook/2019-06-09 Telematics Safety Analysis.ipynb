{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab Safety Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure data visualization\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background and Objective\n",
    "Safety is an important aspect for online transportation. We want customer to feel safe riding Grab so that they could do other things on the way without worry. \n",
    "\n",
    "Customer could feel unsafe because of the driver behaviour or driving skill, eg: \n",
    "- Driver using unpopular shortcuts \n",
    "- Driver talk with other person in phone or with customers\n",
    "- Driver keep seeing GPS and don't pay attention to the road. \n",
    "- Sleepy \n",
    "- Speeding\n",
    "- Harsh acceleration, braking, or cornering\n",
    "- Run over speed bump / hole with high speed.\n",
    "\n",
    "If we could quickly detect when the driver start driving unsafely, we could remind the driver real-time to prevent something bad happend.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "fetaures_raw_files = glob.glob('./data/features/*.csv')\n",
    "featrues_raw_list = []\n",
    "\n",
    "for file_name in fetaures_raw_files[:1]: # remove index to take data from all part. \n",
    "    df = pd.read_csv(file_name, index_col=None, header=0)\n",
    "    featrues_raw_list.append(df)\n",
    "    \n",
    "features_raw = pd.concat(featrues_raw_list, axis=0, ignore_index=True)\n",
    "labels = pd.read_csv('./data/labels/part-00000-e9445087-aa0a-433b-a7f6-7f4c19d78ad6-c000.csv')\n",
    "\n",
    "del featrues_raw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features data sample:')\n",
    "features_raw.head()\n",
    "print('Labels data sample:')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw.loc[features_raw.bookingID==1202590843006,:].sort_values(by='second').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature description\n",
    "### Booking id\n",
    "- Trip id\n",
    "- Possibly relate to service type (GrabCar/Bike) ?\n",
    "\n",
    "### Accuracy\n",
    "- Accuracy inferred by GPS in meters\n",
    "- Affect uncertainty level for GPS Bearing and Speed\n",
    "\n",
    "### Bearing\n",
    "- GPS bearing in degree\n",
    "- The degree of the GPS movement relative from North\n",
    "- Could relate with GPS accuracy. Less accurate means more uncertainty in the real speed.\n",
    "- Beware that 10 degree to 340 degree is 30 degree difference\n",
    "\n",
    "### Acceleration (x, y, z)\n",
    "- Accelerometer reading at certain axis (m/s2)\n",
    "- [Youtube explaination about how Accelerometer works](https://www.youtube.com/watch?v=KZVgKu6v808)\n",
    "- Concern: how could we factor out gravity acceleration.\n",
    "- Concern: phone orientation\n",
    "\n",
    "### Gyro (x, y, z)\n",
    "- Gyroscope reading in certain axis (rad/s)\n",
    "- Measure angular velocity / speed of rotation\n",
    "- [Explaination about how Gyroscope works](https://learn.sparkfun.com/tutorials/gyroscope/all)\n",
    "- Concern: Gyroscope bias, usually caused by heat\n",
    "\n",
    "### Second\n",
    "- Time of the record by number of seconds\n",
    "- Remember that it is not constant, eg: per 2s. If we want to use lag, we need to account for time delta or interpolate it.\n",
    "\n",
    "### Speed\n",
    "- Speed measured by GPS in m/s\n",
    "- Could relate with GPS accuracy. Less accurate means more uncertainty in the real speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train and test set by bookingID\n",
    "We only split the data become two set. Train and test set and we use the test set as validation set as well. \n",
    "The data is split based on `bookingID` to make sure there is no data leak from train to test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_ratio = 0.7\n",
    "\n",
    "all_booking_ids = features_raw.bookingID.unique()\n",
    "np.random.seed(1)\n",
    "train_booking_id = np.random.choice(all_booking_ids, \n",
    "                                    size = int(train_dataset_ratio * all_booking_ids.shape[0]), \n",
    "                                    replace=False)\n",
    "\n",
    "train_dataset = features_raw.loc[features_raw.bookingID.isin(train_booking_id), :].copy(deep=False)\n",
    "train_label = labels.loc[labels.bookingID.isin(train_booking_id), :].copy(deep=False)\n",
    "test_dataset = features_raw.loc[~features_raw.bookingID.isin(train_booking_id), :].copy(deep=False)\n",
    "test_label = labels.loc[~labels.bookingID.isin(train_booking_id), :].copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Safe and un-save trips')\n",
    "labels.label.value_counts()\n",
    "print('\\n')\n",
    "print('#BookingID with more than 1 rows in labels: {0}'.format((labels.bookingID.value_counts() > 1).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle double label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproces_label(labels):\n",
    "    return labels.groupby(['bookingID']).max().reset_index().copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = preproces_label(train_label)\n",
    "test_label = preproces_label(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence and Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_sorted(dataset):\n",
    "    dataset_copy = dataset.copy(deep=False)\n",
    "    \n",
    "    dataset_copy['sequence'] = dataset_copy[\n",
    "        ['bookingID', 'second']\n",
    "    ].groupby('bookingID').rank(ascending=True, method='first')\n",
    "\n",
    "    dataset_copy = dataset_copy.sort_values(by=['bookingID', 'second'])\n",
    "    return dataset_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gyroscope Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually phone is not rotating all the time and the value of gyroscope will be 0.\n",
    "- [Knowing that there is a bias of gyroscope reading](https://base.xsens.com/hc/en-us/articles/209611089-Understanding-Sensor-Bias-offset-), we could use median to find the expected reading while the phone is in stable position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(features_raw.loc[features_raw.bookingID==1477468749954,['gyro_x']])\n",
    "print('Gyroscope reading, x-axis bias:', features_raw.loc[features_raw.bookingID==1477468749954,['gyro_x']].mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gyro_data_enrich(dataset):\n",
    "    enriched_dataset = dataset.copy(deep=False)\n",
    "    enriched_dataset = ensure_sorted(enriched_dataset)\n",
    "    \n",
    "    gyro_cols = ['gyro_x', 'gyro_y', 'gyro_z']\n",
    "    \n",
    "    # Find gyroscope bias / stable values\n",
    "    for col in gyro_cols:\n",
    "        if (col+'_stable') in enriched_dataset.columns:\n",
    "            continue\n",
    "        agg_stable = enriched_dataset.groupby('bookingID')[col].mean().reset_index()\n",
    "        agg_stable.columns = ['bookingID', col+'_stable']\n",
    "        enriched_dataset = pd.merge(enriched_dataset, agg_stable, how='left', on='bookingID', validate='m:1')\n",
    "\n",
    "    # Gyroscope filtered / calibrated values\n",
    "    for col in gyro_cols:\n",
    "        if (col+'_filtered') in enriched_dataset.columns:\n",
    "            continue\n",
    "        enriched_dataset[col+'_filtered'] = enriched_dataset[col] - enriched_dataset[col+'_stable']\n",
    "    \n",
    "    # Gyroscope magnitude of calibrated values\n",
    "    enriched_dataset['gyro_filtered_magnitude'] = np.sqrt(enriched_dataset['gyro_x_filtered']**2 + \\\n",
    "                                                          enriched_dataset['gyro_y_filtered']**2 + \\\n",
    "                                                          enriched_dataset['gyro_z_filtered']**2)\n",
    "    \n",
    "    # Gyroscope magnitude standard deviation\n",
    "    agg_std = enriched_dataset.groupby('bookingID')['gyro_filtered_magnitude'].std().reset_index()\n",
    "    agg_std.columns = ['bookingID', 'gyro_filtered_std']\n",
    "    enriched_dataset = pd.merge(enriched_dataset, agg_std, how='left', on='bookingID', validate='m:1')\n",
    "        \n",
    "    return enriched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gyro_data_enrich(train_dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerometer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accelerometer readings depends on gravity\n",
    "- [Phone orientation](https://www.digikey.com/en/articles/techzone/2011/may/using-an-accelerometer-for-inclination-sensing) could change over time and change the gravity acceleration for each axis\n",
    "\n",
    "Could we?\n",
    "- Handle accelerometer bias? Gravity is not always 9.8. It depends on height and accelerometer bias. \n",
    "- Distinguish between vehicle movement and user moving the phone?\n",
    "- Normalize all data assuming all phones are having the same orientation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accel_data_enrich(dataset, smoothing: int=3):\n",
    "    enriched_dataset = dataset.copy(deep=False)\n",
    "    enriched_dataset = ensure_sorted(enriched_dataset)\n",
    "    \n",
    "    accel_cols = pd.Series(['acceleration_x', 'acceleration_y', 'acceleration_z'])\n",
    "    \n",
    "    # Rolling mean of accleration data to find gravity\n",
    "    rolling_mean_data = enriched_dataset.groupby('bookingID').apply(\n",
    "        lambda x: x[\n",
    "            accel_cols\n",
    "        ].rolling(window=smoothing, min_periods=1, center=True).mean())\n",
    "    rolling_mean_data.columns = accel_cols + '_gravity'\n",
    "    enriched_dataset = pd.concat([enriched_dataset, rolling_mean_data], axis=1, verify_integrity=True)\n",
    "    \n",
    "    # Acceleration magnitude\n",
    "    enriched_dataset['acceleration_magnitude'] = np.sqrt(enriched_dataset['acceleration_x']**2 + \\\n",
    "                                                         enriched_dataset['acceleration_y']**2 + \\\n",
    "                                                         enriched_dataset['acceleration_z']**2) \n",
    "    \n",
    "    # Current acceleration vs gravity diff\n",
    "    for col in accel_cols:\n",
    "        enriched_dataset[col+'_gravity_diff'] = enriched_dataset[col] - enriched_dataset[col+'_gravity']\n",
    "    enriched_dataset['acceleration_gravity_diff_magnitude'] = np.sqrt(enriched_dataset['acceleration_x_gravity_diff']**2 + \\\n",
    "                                                                      enriched_dataset['acceleration_y_gravity_diff']**2 + \\\n",
    "                                                                      enriched_dataset['acceleration_z_gravity_diff']**2) \n",
    "    \n",
    "    # Acceleration magnitude standard deviation\n",
    "    agg_std = enriched_dataset.groupby('bookingID')['acceleration_magnitude', 'acceleration_gravity_diff_magnitude'].std().reset_index()\n",
    "    agg_std.columns = ['bookingID', 'acceleration_std', 'acceleration_gravity_diff_std']\n",
    "    enriched_dataset = pd.merge(enriched_dataset, agg_std, how='left', on='bookingID', validate='m:1')\n",
    "    \n",
    "    # Phone orientation\n",
    "    enriched_dataset['orientation_theta'] = np.arctan(enriched_dataset.acceleration_x_gravity / \\\n",
    "        np.sqrt(enriched_dataset.acceleration_y_gravity**2 + enriched_dataset.acceleration_z_gravity**2)) / np.pi * 360\n",
    "    enriched_dataset['orientation_psi'] = np.arctan(enriched_dataset.acceleration_y_gravity / \\\n",
    "        np.sqrt(enriched_dataset.acceleration_x_gravity**2 + enriched_dataset.acceleration_z_gravity**2)) / np.pi * 360\n",
    "    enriched_dataset['orientation_phi'] = np.arctan( np.sqrt(enriched_dataset.acceleration_x_gravity**2 + enriched_dataset.acceleration_y_gravity**2) / \\\n",
    "        enriched_dataset.acceleration_z_gravity ) / np.pi * 360\n",
    "    \n",
    "    return enriched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accel_data_enrich(train_dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Difference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_data_enrich(dataset):\n",
    "    enriched_dataset = dataset.copy(deep=False)\n",
    "    enriched_dataset = ensure_sorted(enriched_dataset)\n",
    "    \n",
    "    # Construct diff\n",
    "    diff_data = enriched_dataset.groupby('bookingID')['second','Bearing','Speed'].diff()\n",
    "    diff_data = diff_data.rename(columns = lambda x: x + '_diff')\n",
    "    \n",
    "    # Modify Bearing diff to -180 to 180 \n",
    "    diff_data.Bearing_diff = diff_data.Bearing_diff\n",
    "    diff_data.Bearing_diff[diff_data.Bearing_diff < -180.0] += 180\n",
    "    diff_data.Bearing_diff[diff_data.Bearing_diff > 180.0] -= 180\n",
    "\n",
    "    # Difference / second (normalization)\n",
    "    diff_data['Bearing_dps'] = diff_data['Bearing_diff'] / diff_data['second_diff']\n",
    "    diff_data['Speed_dps'] = diff_data['Speed_diff'] / diff_data['second_diff']\n",
    "    \n",
    "    # Combine\n",
    "    diff_data = diff_data.fillna(0)\n",
    "    enriched_dataset = pd.concat([enriched_dataset, diff_data], axis=1, verify_integrity=True)\n",
    "    \n",
    "    # Combine accuracy of two sequence\n",
    "    acc_sum = enriched_dataset.groupby('bookingID')['Accuracy']\\\n",
    "       .rolling(window=2, min_periods=1).sum().reset_index(drop=True).tolist()\n",
    "    enriched_dataset['Accuracy_sum'] = acc_sum\n",
    "    \n",
    "    return enriched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# diff_data_enrich(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    dataset = gyro_data_enrich(dataset)\n",
    "    dataset = accel_data_enrich(dataset, smoothing=5)\n",
    "    dataset = diff_data_enrich(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_prep = preprocess(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chart_trip(dataset, booking_id):\n",
    "    booking_id_data = dataset.loc[dataset.bookingID==booking_id,:].sort_values(by='second')\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplots_adjust(hspace = .001)\n",
    "    \n",
    "    # Acceleration\n",
    "    booking_id_acc = booking_id_data[\n",
    "        ['second','acceleration_x', 'acceleration_y','acceleration_z', 'acceleration_magnitude']\n",
    "    ].melt(id_vars=[\"second\"], var_name=\"axis\", value_name=\"value\")\n",
    "    ax1 = plt.subplot('311')\n",
    "    \n",
    "    plt.title(\"Measured data for booking ID: {}\".format(booking_id))\n",
    "    sns.lineplot(x=\"second\", y=\"value\", hue='axis', data=booking_id_acc, ax=ax1, marker=\"o\");\n",
    "    \n",
    "    # Gyroscope\n",
    "    booking_id_gyro = booking_id_data[\n",
    "        ['second', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    ].melt(id_vars=[\"second\"], var_name=\"axis\", value_name=\"value\")\n",
    "    ax2 = plt.subplot('312')\n",
    "    sns.lineplot(x=\"second\", y=\"value\", hue='axis', data=booking_id_gyro, ax=ax2, markers=True, marker=\"o\");\n",
    "    \n",
    "    # Speed\n",
    "    booking_id_speed = booking_id_data[\n",
    "        ['second', 'Speed', 'Accuracy']\n",
    "    ].melt(id_vars=[\"second\"], var_name=\"type\", value_name=\"value\")\n",
    "    ax3 = plt.subplot('313')\n",
    "    sns.lineplot(x='second', y=\"value\", hue='type', data=booking_id_speed, ax=ax3, markers=True, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_trip(train_dataset_prep, 1477468749954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of non-safe trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = train_label.bookingID[train_label.label == 1].sample(5, random_state=1)\n",
    "for id in samp:\n",
    "    chart_trip(train_dataset_prep, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of safe trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = train_label.bookingID[train_label.label == 0].sample(5, random_state=1)\n",
    "for id in samp:\n",
    "    chart_trip(train_dataset_prep, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analytics_features = ['gyro_filtered_magnitude',\n",
    "                      'acceleration_magnitude',\n",
    "                      'Speed',\n",
    "                      'Bearing_dps',\n",
    "                      'Speed_dps',\n",
    "                      'second_diff',\n",
    "                      'second',\n",
    "                      'Accuracy_sum',\n",
    "                      'acceleration_x', \n",
    "                      'acceleration_y', \n",
    "                      'acceleration_z',\n",
    "                      'acceleration_x_gravity_diff',\n",
    "                      'acceleration_y_gravity_diff',\n",
    "                      'acceleration_z_gravity_diff',\n",
    "                      'acceleration_gravity_diff_magnitude',\n",
    "                      'gyro_x_filtered',\n",
    "                      'gyro_y_filtered',\n",
    "                      'gyro_z_filtered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analytics_mean_corr = train_dataset_prep.groupby('bookingID')[analytics_features].mean().reset_index()\n",
    "analytics_mean_corr = pd.merge(analytics_mean_corr, train_label, on='bookingID')\n",
    "analytics_mean_corr = analytics_mean_corr.corr()\n",
    "\n",
    "plt.figure( figsize=(15,5) )\n",
    "ax1 = plt.subplot2grid((1, 4), (0, 0), colspan=2)\n",
    "ax2 = plt.subplot2grid((1, 4), (0, 3), colspan=1)\n",
    "sns.heatmap(analytics_mean_corr, ax=ax1)\n",
    "sns.heatmap(pd.DataFrame(analytics_mean_corr.loc[analytics_mean_corr.index != 'label','label']), annot=True, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation show that mean / average data is less effective to determine a trip is save or unsafe. The unsafe tracking event maybe only recorded 1 time or maybe < 5% of the trip. But, acceleration and gyroscope magnitude data stand out here. How many % of the trips where the driver consistently drive unsafely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_max_corr = train_dataset_prep.groupby('bookingID')[analytics_features].max().reset_index()\n",
    "analytics_max_corr = pd.merge(analytics_max_corr, train_label, on='bookingID')\n",
    "analytics_max_corr = analytics_max_corr.corr()\n",
    "\n",
    "plt.figure( figsize=(15,5) )\n",
    "ax1 = plt.subplot2grid((1, 4), (0, 0), colspan=2)\n",
    "ax2 = plt.subplot2grid((1, 4), (0, 3), colspan=1)\n",
    "sns.heatmap(analytics_max_corr, ax=ax1)\n",
    "sns.heatmap(pd.DataFrame(analytics_max_corr.loc[analytics_max_corr.index != 'label','label']), annot=True, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard deviation correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_std_corr = train_dataset_prep.groupby('bookingID')[analytics_features].std().reset_index()\n",
    "analytics_std_corr = pd.merge(analytics_std_corr, train_label, on='bookingID')\n",
    "analytics_std_corr = analytics_std_corr.corr()\n",
    "\n",
    "plt.figure( figsize=(15,5) )\n",
    "ax1 = plt.subplot2grid((1, 4), (0, 0), colspan=2)\n",
    "ax2 = plt.subplot2grid((1, 4), (0, 3), colspan=1)\n",
    "sns.heatmap(analytics_std_corr, ax=ax1)\n",
    "sns.heatmap(pd.DataFrame(analytics_std_corr.loc[analytics_std_corr.index != 'label','label']), annot=True, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gyro and acceleration standard deviation is highly correlated. The hypothesis is when there is angular velocity (gyroscope reading), the phone orientation is changing. Hence, the direction of gravity relative to the phone is changing and the reading for each accelerometer axis is changing too. \n",
    "\n",
    "Because it is highly correlated, we will just use subset of it:\n",
    "- gyroscope filter magnitude's standard deviation\n",
    "- acceleration gravity diff magnitude's standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why accelerometer and gyroscope magnitude have high correlation with label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_gyro_mean = train_dataset_prep.groupby('bookingID')[\n",
    "    'acceleration_gravity_diff_magnitude', \n",
    "    'gyro_filtered_magnitude'\n",
    "].mean().reset_index()\n",
    "\n",
    "acc_gyro_mean = pd.merge(acc_gyro_mean, train_label, on='bookingID', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(15,6))\n",
    "\n",
    "ax1 = plt.subplot('121')\n",
    "sns.scatterplot(x=\"acceleration_gravity_diff_magnitude\", \n",
    "                y=\"gyro_filtered_magnitude\", \n",
    "                hue=\"label\", \n",
    "                data=acc_gyro_mean.sample(1000, random_state=1),\n",
    "                ax=ax1);\n",
    "\n",
    "ax2 = plt.subplot('122')\n",
    "sns.scatterplot(x=\"acceleration_gravity_diff_magnitude\", \n",
    "                y=\"gyro_filtered_magnitude\", \n",
    "                hue=\"label\", \n",
    "                data=acc_gyro_mean.sample(500, random_state=1),\n",
    "                ax=ax2);\n",
    "plt.xlim(0, 2);\n",
    "plt.ylim(0, 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Could we cluster the trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_data(preprocessed_dataset):\n",
    "    features_max = ['gyro_filtered_magnitude',\n",
    "                    'acceleration_magnitude',\n",
    "                    'Speed',\n",
    "                    'Bearing_dps',\n",
    "                    'Speed_dps',\n",
    "                    'Accuracy_sum',\n",
    "                    'second',\n",
    "                    'sequence',\n",
    "                    'acceleration_x_gravity_diff',\n",
    "                    'acceleration_y_gravity_diff',\n",
    "                    'acceleration_z_gravity_diff',\n",
    "                    'acceleration_gravity_diff_magnitude',\n",
    "                    'gyro_x_filtered',\n",
    "                    'gyro_y_filtered',\n",
    "                    'gyro_z_filtered']\n",
    "    \n",
    "    agg_max = preprocessed_dataset.groupby('bookingID')[features_max].max().reset_index()\n",
    "    agg_max.columns = ['bookingID'] +  (pd.Series(features_max) + '_max').to_list()\n",
    "\n",
    "    features_std = ['gyro_filtered_magnitude', 'acceleration_gravity_diff_magnitude']\n",
    "    agg_std = preprocessed_dataset.groupby('bookingID')[features_std].std().reset_index()\n",
    "    agg_std.columns = ['bookingID'] +  (pd.Series(features_std) + '_std').to_list()\n",
    "    \n",
    "    agg_data = pd.merge(agg_max, agg_std, on='bookingID', validate='1:1')\n",
    "    agg_data['second_sequence_ratio'] = agg_data['second_max'] / agg_data['sequence_max'].astype(float)\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg_data = aggregate_data(train_dataset_prep)\n",
    "train_agg_data = pd.merge(train_agg_data, train_label, on='bookingID', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = train_agg_data.columns[train_agg_data.columns.str.contains(\"max|std\")]\n",
    "\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "x = std_scaler.fit_transform(train_agg_data[features])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pc = pca.fit_transform(x)\n",
    "pc_df = pd.DataFrame(data = pc, columns = ['pc1', 'pc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=\"pc1\", \n",
    "                y=\"pc2\", \n",
    "                hue=\"label\",\n",
    "                data=pd.concat([train_agg_data, pc_df], axis=1, verify_integrity=True).sample(2000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **no visually standout clusters** after we reduce the features to 2-dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Our goal is to make the model which is capable to find the pattern of non-safe event like:\n",
    "- Driver using unpopular shortcuts \n",
    "- Driver talk with other person in phone or with customers\n",
    "- Driver keep seeing GPS and don't pay attention to the road. \n",
    "- Sleepy \n",
    "- Speeding\n",
    "- Harsh acceleration, braking, or cornering\n",
    "- Run over speed bump / hole with high speed.\n",
    "\n",
    "There are 3 rough idea to approach this problem:\n",
    "1. Learning on the summary of a trip\n",
    "2. Learning directly from the trip sequence with reccurent model. Assume it is like NLP sentiment classification with recurrent neural network. \n",
    "3. Stacking two models. First model to detect non-safe event. Second model to summarize it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def combine_pred_label(prediction_df, label_df):\n",
    "    \"\"\"Combine two DataFrame, each DataFrame should contains 'bookingID' column.\"\"\"\n",
    "    return pd.merge(prediction_df, test_label, how='left', on='bookingID', validate='1:1')\n",
    "\n",
    "def plot_roc(prediction_df, label_df):\n",
    "    \"\"\"Return ROC plot given prediction and label DataFrame. Both should have 'bookingID' column.\"\"\"\n",
    "    pred_label_df = combine_pred_label(prediction_df, label_df)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(pred_label_df.label, pred_label_df.prediction)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate(prediction_df, label_df):\n",
    "    \"\"\"Return AUC evaluation given prediction and label DataFrame. Both should have 'bookingID' column.\"\"\"\n",
    "    pred_label_df = combine_pred_label(prediction_df, label_df)\n",
    "    return roc_auc_score(pred_label_df.label, pred_label_df.prediction)\n",
    "                         \n",
    "def generate_second_dataset(dataset, prediciton, n_column=5):\n",
    "    \"\"\"\n",
    "    Generate per booking secondary dataset from per event unsafeness prediction.\n",
    "    Return top n_columns unsafeness for each bookingID\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataset -- DataFrame contains bookingID\n",
    "    prediction -- List / Series with length equal to dataset # rows. Each indicates unsafeness. \n",
    "    n_column -- number of column generated\n",
    "    \"\"\"\n",
    "    sec_data = pd.DataFrame(data={'bookingID':dataset.bookingID, 'row_prob': prediciton})\n",
    "    sec_data['rank'] = sec_data.groupby('bookingID').rank(ascending=False, method='first')\n",
    "    sec_data = sec_data.loc[sec_data['rank'] <= n_column, :]\n",
    "    sec_data = pd.pivot_table(data=sec_data, \n",
    "                              values='row_prob', \n",
    "                              index='bookingID', \n",
    "                              columns='rank', \n",
    "                              fill_value=0).reset_index()\n",
    "    sec_data.columns=['bookingID'] + ['val_' + str(i) for i in range(1, (n_column + 1))]\n",
    "    return sec_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "prediction_df = pd.DataFrame({\n",
    "    'bookingID': test_dataset.bookingID.unique(),\n",
    "    'prediction': np.random.random(size=test_dataset.bookingID.unique().shape[0])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(prediction_df, test_label)\n",
    "plot_roc(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Aggregated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_prep = preprocess(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_agg_data = aggregate_data(train_dataset_prep)\n",
    "train_agg_data = pd.merge(train_agg_data, train_label, on='bookingID', validate='1:1')\n",
    "\n",
    "test_dataset_prep = preprocess(test_dataset)\n",
    "test_agg_data = aggregate_data(test_dataset_prep)\n",
    "\n",
    "features = train_agg_data.columns[train_agg_data.columns.str.contains(\"max|std|ratio\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = RandomForestClassifier(n_estimators=100, random_state=0, min_samples_leaf=75)\n",
    "cls.fit(train_agg_data[features], train_agg_data.label)\n",
    "pred = cls.predict_proba(test_agg_data[features])\n",
    "pred = pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "prediction_df = pd.DataFrame(data={'bookingID':test_agg_data.bookingID, 'prediction': pred})\n",
    "print('AUC:',evaluate(prediction_df, test_label))\n",
    "plot_roc(prediction_df, test_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "temp = combine_pred_label(prediction_df=prediction_df, label_df=test_label)\n",
    "temp['pred'] = (temp.prediction >= 0.66).astype(int)\n",
    "pd.crosstab(temp.label, temp.pred)\n",
    "recall_score(temp.label, temp.pred)\n",
    "precision_score(temp.label, temp.pred)\n",
    "f1_score(temp.label, temp.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_prep = preprocess(train_dataset)\n",
    "train_dataset_prep = pd.merge(train_dataset_prep, train_label, on='bookingID', validate='m:1')\n",
    "test_dataset_prep = preprocess(test_dataset)\n",
    "test_dataset_prep = pd.merge(test_dataset_prep, test_label, on='bookingID', validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Accuracy', 'Bearing', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'gyro_x_filtered', \n",
    "            'gyro_y_filtered', 'gyro_z_filtered', 'Speed', 'gyro_filtered_magnitude', 'gyro_filtered_std',\n",
    "            'acceleration_x_gravity', 'acceleration_y_gravity', 'acceleration_z_gravity', 'acceleration_magnitude',\n",
    "            'orientation_theta', 'orientation_psi', 'orientation_phi', 'Bearing_dps', 'Speed_dps', 'Accuracy_sum']\n",
    "\n",
    "# features = ['Accuracy', 'Bearing', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'Speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = RandomForestClassifier(n_estimators=10, random_state=0, min_samples_leaf=500)\n",
    "cls.fit(train_dataset_prep[features], train_dataset_prep.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this\n",
    "y_pred = cls.predict_proba(test_dataset_prep[features])\n",
    "y_pred = y_pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "roc_auc_score(test_dataset_prep.label, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred_first = cls.predict_proba(train_dataset_prep[features])\n",
    "train_pred_first = train_pred_first[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "\n",
    "test_pred_first = cls.predict_proba(test_dataset_prep[features])\n",
    "test_pred_first = test_pred_first[:,np.argwhere(cls.classes_==1)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_sec = generate_second_dataset(train_dataset_prep, train_pred_first, n_column=5)\n",
    "train_sec = pd.merge(train_sec, train_label, on='bookingID')\n",
    "sec_features = train_sec.columns[train_sec.columns.str.contains('val')]\n",
    "sec_reg = LogisticRegression(random_state=0, penalty='l2', C=0.1)\n",
    "sec_reg.fit(train_sec[sec_features], train_sec.label)\n",
    "\n",
    "\n",
    "test_sec = generate_second_dataset(test_dataset_prep, test_pred_first, n_column=5)\n",
    "test_sec = pd.merge(test_sec, test_label, on='bookingID')\n",
    "y_pred = sec_reg.predict_proba(test_sec[sec_features])\n",
    "y_pred = y_pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "prediction_df = pd.DataFrame(data={'bookingID':test_sec.bookingID, 'prediction': y_pred})\n",
    "\n",
    "\n",
    "evaluate(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_prep = preprocess(train_dataset)\n",
    "train_dataset_prep = pd.merge(train_dataset_prep, train_label, on='bookingID', validate='m:1')\n",
    "test_dataset_prep = preprocess(test_dataset)\n",
    "test_dataset_prep = pd.merge(test_dataset_prep, test_label, on='bookingID', validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prep.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features = ['Accuracy', 'acceleration_x', 'acceleration_y', 'acceleration_z', 'Speed', 'gyro_x_filtered', \n",
    "#             'gyro_y_filtered', 'gyro_z_filtered', 'gyro_filtered_magnitude', 'gyro_filtered_std',\n",
    "#             'acceleration_x_gravity', 'acceleration_y_gravity', 'acceleration_z_gravity', 'acceleration_magnitude',\n",
    "#             'acceleration_x_gravity_diff', 'acceleration_y_gravity_diff', 'acceleration_z_gravity_diff', \n",
    "#             'acceleration_z_gravity_diff', 'acceleration_gravity_diff_magnitude', 'acceleration_std',\n",
    "#             'acceleration_gravity_diff_magnitude_std', 'Bearing_dps', 'Speed_dps', 'Accuracy_sum', 'second_diff']\n",
    "\n",
    "features = ['Accuracy', 'gyro_x_filtered', 'gyro_y_filtered', 'gyro_z_filtered', 'Speed', 'gyro_filtered_magnitude', \n",
    "            'acceleration_magnitude', 'Bearing_dps', 'Speed_dps', 'Accuracy_sum', \n",
    "            'acceleration_x_gravity_diff', 'acceleration_y_gravity_diff', 'acceleration_z_gravity_diff', \n",
    "            'acceleration_gravity_diff_magnitude', 'orientation_theta', \n",
    "            'orientation_psi', 'orientation_phi', 'second'] + \\\n",
    "            ['acceleration_gravity_diff_std','acceleration_std','gyro_filtered_std']\n",
    "\n",
    "# features = ['Accuracy', 'Bearing', 'acceleration_x', 'acceleration_y', \n",
    "#             'acceleration_z', 'gyro_x', 'gyro_y', 'gyro_z', 'Speed', 'second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# gridParams = {\n",
    "#     # Efficiency\n",
    "#     \"num_leaves\": [70], # more for accuracy, beware overfitting, max to 2^(max_depth)\n",
    "#     \"min_data_in_leaf\": [150],\n",
    "#     \"max_depth\": [6], \n",
    "#     \"n_estimators\": [100],\n",
    "#     # Speed\n",
    "# #     \"bagging_fraction\": [1.0, 0.9],\n",
    "# #     \"bagging_freq\": [5],\n",
    "# #     \"feature_fraction\": [0.8, 0.9, 1.0],\n",
    "# #     \"subsample\":[1.0, 0.8],\n",
    "#     \"max_bin\": [125], # more bin for accuration,\n",
    "#     # Accuracy\n",
    "#     \"boosting_type\": ['dart'],\n",
    "# #     # Overfit\n",
    "#     \"lambda_l1\": [0.4],\n",
    "#     # Others\n",
    "#     \"learning_rate\": [0.05, 0.1, 0.15]\n",
    "# }\n",
    "\n",
    "# cslf = lgb.LGBMClassifier(boosting_type='gbdt', \n",
    "#                           objective='binary', \n",
    "#                           max_depth=6, \n",
    "#                           learning_rate=0.1,\n",
    "#                           n_estimators=100,\n",
    "#                           num_leaves=70,  \n",
    "#                           metric='auc',\n",
    "#                           random_state=10)\n",
    "\n",
    "# grid_search = GridSearchCV(cslf, gridParams, n_jobs=4, verbose=2, return_train_score=False)\n",
    "# grid_search.fit(train_dataset_prep[features], train_dataset_prep.label)\n",
    "\n",
    "# best_parameters = grid_search.best_estimator_.get_params()\n",
    "# best_parameters\n",
    "\n",
    "# cv_result = pd.DataFrame(grid_search.cv_results_)\n",
    "# cv_result = cv_result.loc[:,('params', 'rank_test_score', 'mean_test_score')]\n",
    "# with pd.option_context('display.max_colwidth', -1):\n",
    "#     cv_result.sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = lgb.LGBMClassifier(boosting_type='dart', \n",
    "                         objective='binary', \n",
    "                         max_depth=6, \n",
    "                         n_estimator=100,\n",
    "                         learning_rate=0.1, \n",
    "                         max_bin=100, \n",
    "                         num_leaves=70, \n",
    "                         lambda_l1=0.4,\n",
    "                         min_data_in_leaf=150,\n",
    "                         metric='auc')\n",
    "\n",
    "cls.fit(train_dataset_prep[features], train_dataset_prep.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this\n",
    "y_pred = cls.predict_proba(test_dataset_prep[features])\n",
    "y_pred = y_pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "roc_auc_score(test_dataset_prep.label, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred_first = cls.predict_proba(train_dataset_prep[features])\n",
    "train_pred_first = train_pred_first[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "\n",
    "test_pred_first = cls.predict_proba(test_dataset_prep[features])\n",
    "test_pred_first = test_pred_first[:,np.argwhere(cls.classes_==1)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_sec = generate_second_dataset(train_dataset_prep, train_pred_first, n_column=5)\n",
    "train_sec = pd.merge(train_sec, train_label, on='bookingID')\n",
    "sec_features = train_sec.columns[train_sec.columns.str.contains('val')]\n",
    "sec_reg = LogisticRegression(random_state=0, penalty='l2', C=0.1)\n",
    "sec_reg.fit(train_sec[sec_features], train_sec.label)\n",
    "\n",
    "test_sec = generate_second_dataset(test_dataset_prep, test_pred_first, n_column=5)\n",
    "test_sec = pd.merge(test_sec, test_label, on='bookingID')\n",
    "y_pred = sec_reg.predict_proba(test_sec[sec_features])\n",
    "y_pred = y_pred[:,np.argwhere(cls.classes_==1)[0][0]]\n",
    "prediction_df = pd.DataFrame(data={'bookingID':test_sec.bookingID, 'prediction': y_pred})\n",
    "\n",
    "evaluate(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(prediction_df, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(prediction_df.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "temp = combine_pred_label(prediction_df=prediction_df, label_df=test_label)\n",
    "temp['pred'] = (temp.prediction >= 0.6).astype(int)\n",
    "pd.crosstab(temp.label, temp.pred)\n",
    "recall_score(temp.label, temp.pred)\n",
    "precision_score(temp.label, temp.pred)\n",
    "f1_score(temp.label, temp.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
